# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-TNLl9PClAqC9WxHH8NMek3v3uQNjmDX
"""

# [RUN THIS CELL AT THE VERY TOP OF YOUR NOTEBOOK]

import tensorflow as tf
import numpy as np
import datetime
import matplotlib.pyplot as plt
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.layers import RandomFlip, RandomRotation


from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Layer,
    Dense,
    Conv1D,
    Conv2D,
    LayerNormalization,
    Activation,
    Embedding,
    Dropout,
    GlobalAveragePooling1D,
    Input
)
from einops.layers.tensorflow import Rearrange

print("✅ All necessary libraries imported.")

# [USE THIS CORRECTED CELL TO PREPARE THE PneumoniaMNIST DATASET]
import numpy as np
import tensorflow as tf
from google.colab import drive

# 1. Mount Google Drive
drive.mount('/content/drive')

# 2. Define the path to your dataset on Google Drive
dataset_path = '/content/drive/My Drive/datasets/pneumoniamnist_224.npz'

# 3. Load the data from the .npz file
print("Loading data from .npz file...")
pneumonia_data = np.load(dataset_path)

# 4. Extract the image and label arrays
X_train = pneumonia_data['train_images']
y_train = pneumonia_data['train_labels']
X_val = pneumonia_data['val_images']
y_val = pneumonia_data['val_labels']
print("Data arrays extracted.")

# 5. --- THIS IS THE CORRECTED PART ---
# First, add a channel dimension to the images (shape becomes 224, 224, 1)
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
# Then, repeat the channel dimension 3 times (shape becomes 224, 224, 3)
X_train = np.repeat(X_train, 3, axis=-1)
X_val = np.repeat(X_val, 3, axis=-1)
# ------------------------------------

print(f"Final data shapes: X_train: {X_train.shape}, X_val: {X_val.shape}")

# 6. Create memory-efficient TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))

# 7. Configure datasets for performance
batch_size = 16
AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.shuffle(1000).batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)
val_dataset = val_dataset.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)

# 8. Define class names manually
class_names = ['normal', 'pneumonia']
print(f"Using classes: {class_names}")

print("\n✅ PneumoniaMNIST datasets created successfully!")

# [USE THIS FINAL VERSION OF MAMBABLOCK]

class MambaBlock(Layer):
    def __init__(
        self,
        d_state=16,
        d_conv=4,
        expand=2,
        **kwargs,  # Add **kwargs here
    ):
        super().__init__(**kwargs)
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        self.d_inner = int(self.expand * self.d_model)

        self.norm = LayerNormalization()
        self.in_proj = Dense(self.d_inner * 2, activation=None)

        self.conv1d = Conv1D(
            filters=self.d_inner, kernel_size=self.d_conv, strides=1,
            padding="same", # Use "same" to avoid compiler issues
            groups=self.d_inner, activation=None
        )

        self.dt_proj = Dense(self.d_inner, activation=None)
        self.B_proj = Dense(self.d_state, activation=None)
        self.C_proj = Dense(self.d_state, activation=None)
        self.out_proj = Dense(self.d_model, activation=None)

        A = tf.experimental.numpy.arange(1, self.d_state + 1, dtype=tf.float32)
        A = tf.tile(tf.expand_dims(A, 0), [self.d_inner, 1])
        self.A_log = self.add_weight(shape=A.shape, initializer=lambda shape, dtype: tf.math.log(A), trainable=True, name="A_log")
        self.D = self.add_weight(shape=(self.d_inner,), initializer="ones", trainable=True, name="D")

        super().build(input_shape)

    def ssm(self, x):
        A = -tf.exp(tf.cast(self.A_log, dtype=tf.float32))
        D = tf.cast(self.D, dtype=tf.float32)
        delta = tf.nn.softplus(self.dt_proj(x))
        B = self.B_proj(x)
        C = self.C_proj(x)

        dA = tf.exp(tf.einsum('b l d, d n -> b l d n', delta, A))
        dB = tf.einsum('b l d, b l n -> b l d n', delta, B)

        dA_transposed = tf.transpose(dA, perm=[1, 0, 2, 3])
        dB_transposed = tf.transpose(dB, perm=[1, 0, 2, 3])

        h = tf.scan(
            lambda h_prev, elems: h_prev * elems[0] + elems[1],
            (dA_transposed, dB_transposed),
            initializer=tf.zeros([tf.shape(x)[0], self.d_inner, self.d_state])
        )

        h = tf.transpose(h, perm=[1, 0, 2, 3])

        y = tf.einsum('b l d n, b l n -> b l d', h, C) + x * tf.expand_dims(D, axis=0)
        return y

    def call(self, x):
        x_residual = x
        x = self.norm(x)
        x_proj = self.in_proj(x)
        x_intermediate, gate = tf.split(x_proj, num_or_size_splits=2, axis=-1)
        x_conv = self.conv1d(x_intermediate)
        x_activated = tf.nn.silu(x_conv)
        y_ssm = self.ssm(x_activated)
        y_gated = y_ssm * tf.nn.silu(gate)
        y_out = self.out_proj(y_gated)
        return x_residual + y_out
    # ADD THIS METHOD TO YOUR CLASS
    def get_config(self):
        config = super().get_config()
        config.update({
            "d_state": self.d_state,
            "d_conv": self.d_conv,
            "expand": self.expand,
        })
        return config

# [USE THIS UPDATED FUNCTION TO CREATE YOUR MODEL]

def create_medmamba_model(input_shape=(224,224,3)):
    inputs = Input(shape=input_shape)
# Add this data augmentation block
    x = RandomFlip("horizontal")(inputs)
    x = RandomRotation(0.1)(x)
    # Enhanced patch embedding
    x = Conv2D(64, (16,16), strides=16, padding='valid')(x)
    x = LayerNormalization()(x)
    x = Activation('gelu')(x)

    # Sequence processing
    x = Rearrange('b h w c -> b (h w) c')(x)

    # Add positional embeddings
    positions = tf.range(start=0, limit=196, delta=1)
    pos_embed = Embedding(input_dim=196, output_dim=64)(positions)
    x = x + pos_embed

    # Mamba blocks with skip connections
    for _ in range(4):
        x_res = x
        x = MambaBlock(d_state=16, expand=2)(x)
        x = Dropout(0.1)(x)
        x = x_res + x

    # Enhanced classifier
    x = GlobalAveragePooling1D()(x)
    x = Dense(128, activation='gelu')(x)
    x = Dropout(0.3)(x)

    # --- THIS IS THE REQUIRED CHANGE ---
    # Final layer for binary classification (2 classes)
    outputs = Dense(1, activation='sigmoid')(x)

    return Model(inputs, outputs)

# [THIS IS THE FINAL CELL TO RUN]

# 1. Create the improved model
# This assumes you have already run the cell with the updated create_medmamba_model function
model = create_medmamba_model()
model.summary()

# 2. Compile with the correct loss function for binary classification
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='binary_crossentropy',  # --- THIS IS THE REQUIRED CHANGE ---
    metrics=['accuracy']
)

# 3. Define callbacks
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', mode='min', restore_best_weights=True)
model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_medmamba_model.keras', save_best_only=True, monitor='val_loss', mode='min')


# 4. Train the model
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=val_dataset,
    callbacks=[tensorboard_callback, early_stopping, model_checkpoint]
)

# 5. Generate and plot visualizations
def plot_metrics(history):
    plt.figure(figsize=(12, 5))

    # Plot Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy')
    plt.legend()

    # Plot Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_metrics.png')
    plt.show()

plot_metrics(history)

# Create a folder in your Google Drive if it doesn't exist
!mkdir -p "/content/drive/My Drive/saved_models/"

# Copy the saved model from the temporary session to your Google Drive
!cp best_medmamba_model.keras "/content/drive/My Drive/saved_models/"

print("✅ Model successfully copied to Google Drive!")

!ls "/content/drive/My Drive/saved_models/"

# [FINAL EVALUATION SCRIPT FOR PneumoniaMNIST]

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
import warnings

from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model
from einops.layers.tensorflow import Rearrange

# Suppress the harmless UserWarning from Keras
warnings.filterwarnings('ignore', category=UserWarning)

# 1. Load the best saved model from Google Drive
model_path = '/content/drive/My Drive/saved_models/best_medmamba_model.keras'
best_model = load_model(
    model_path,
    custom_objects={
        'MambaBlock': MambaBlock,
        'Rearrange': Rearrange
    }
)
print("✅ Best model loaded successfully from Google Drive!")

# (The rest of the evaluation script remains the same...)

# 2. Function to get labels and predictions for the binary model
def get_labels_and_predictions(dataset, model):
    y_true = []
    y_pred_probs = []
    for images, labels in dataset.as_numpy_iterator():
        y_true.extend(labels)
        y_pred_probs.extend(model.predict(images, verbose=0))

    y_true = np.array(y_true).flatten()
    y_pred_probs = np.array(y_pred_probs).flatten()
    y_pred = (y_pred_probs > 0.5).astype(int)
    return y_true, y_pred, y_pred_probs

# Get predictions for the validation set
y_true_val, y_pred_val, y_probs_val = get_labels_and_predictions(val_dataset, best_model)

# 3. Plot Confusion Matrix
def plot_confusion_matrix(y_true, y_pred, class_names, title='Confusion Matrix'):
    cm = confusion_matrix(y_true, y_pred)
    cm_percent = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-7) * 100

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Percentage'})
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

plot_confusion_matrix(y_true_val, y_pred_val, class_names, 'Validation Confusion Matrix (%)')

# 4. Print Classification Report and AUC Score
print("\nValidation Classification Report:")
print(classification_report(y_true_val, y_pred_val, target_names=class_names))
print(f"Validation AUC: {roc_auc_score(y_true_val, y_probs_val):.4f}")

# 5. Plot ROC Curve
def plot_roc_curve(y_true, y_score, title='ROC Curve'):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    plt.show()

plot_roc_curve(y_true_val, y_probs_val, 'Validation ROC Curve')

# 6. FLOPs Calculation
try:
    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
    concrete = tf.function(lambda inputs: best_model(inputs))
    concrete_func = concrete.get_concrete_function(
        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in best_model.inputs])
    frozen_func = convert_variables_to_constants_v2(concrete_func)
    graph = frozen_func.graph

    flops = tf.compat.v1.profiler.profile(
        graph,
        options=tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()
    )
    print(f"\nModel FLOPS: {flops.total_float_ops:,} (~{flops.total_float_ops/1e9:.2f} GFLOPS)")
except Exception as e:
    print(f"\nCould not calculate FLOPS. You can use Total Params from model.summary() instead.")
    print(f"Error: {e}")

# 7. Grad-CAM Visualization
def get_gradcam(model, img_array, layer_name):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        loss = predictions[0]

    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + tf.keras.backend.epsilon())
    return heatmap.numpy()

def plot_gradcam_examples(dataset, model, class_names, layer_name, n_examples=3):
    plt.figure(figsize=(12, 5 * n_examples))
    for images, labels in dataset.take(1):
        for i in range(min(n_examples, len(images))):
            img = images[i].numpy()
            img_array = np.expand_dims(img, axis=0)

            heatmap = get_gradcam(model, img_array, layer_name)
            heatmap = tf.image.resize(np.expand_dims(heatmap, axis=-1), img.shape[:2]).numpy().squeeze()

            pred_prob = model.predict(img_array, verbose=0)[0][0]
            pred_class = class_names[1] if pred_prob > 0.5 else class_names[0]

            plt.subplot(n_examples, 2, 2*i + 1)
            plt.imshow(img.astype('uint8'))
            plt.title(f"True: {class_names[int(labels[i])]}\nPred: {pred_class} ({pred_prob:.2f})")
            plt.axis('off')

            plt.subplot(n_examples, 2, 2*i + 2)
            plt.imshow(img.astype('uint8'))
            plt.imshow(heatmap, cmap='jet', alpha=0.4)
            plt.title('Grad-CAM Heatmap')
            plt.axis('off')
    plt.tight_layout()
    plt.show()

first_conv_layer_name = None
for layer in best_model.layers:
    if isinstance(layer, tf.keras.layers.Conv2D):
        first_conv_layer_name = layer.name
        break
if first_conv_layer_name:
    print(f"\nGenerating Grad-CAM visualizations for layer: {first_conv_layer_name}...")
    plot_gradcam_examples(val_dataset, best_model, class_names, first_conv_layer_name)
else:
    print("Could not find a Conv2D layer for Grad-CAM.")

import matplotlib.pyplot as plt

# --- Your Final Model Metrics ---
# (Pulled from your successful evaluation run)
val_accuracy = 0.88  # 88%
val_auc = 0.84       # From the ROC Curve
total_params = 251717 # From your model.summary()

# --- Create the Visualization ---
fig, ax = plt.subplots(figsize=(8, 5))

# Hide the axes
ax.axis('off')
ax.set_title("MedMamba Final Performance (PneumoniaMNIST)", fontsize=20, pad=20)

# Add the metrics as text
ax.text(0.5, 0.7, f"Validation Accuracy: {val_accuracy:.2%}",
        ha='center', va='center', fontsize=18, color='green')

ax.text(0.5, 0.5, f"Validation AUC: {val_auc:.4f}",
        ha='center', va='center', fontsize=18)

ax.text(0.5, 0.3, f"Total Parameters: {total_params:,}",
        ha='center', va='center', fontsize=18)

# Save and show the plot
plt.savefig('final_performance_summary.png', bbox_inches='tight')
plt.show()

# %% [markdown]
# # MedMamba vs RankSVM Comparison (With Original Mamba Implementation)

# %%
# Install required packages
!pip install seaborn pandas scikit-learn einops

# %%
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Conv1D, LayerNormalization, Input, Conv2D, Dropout
from tensorflow.keras.layers import RandomFlip, RandomRotation, Activation, Embedding, GlobalAveragePooling1D
from tensorflow.keras.models import Model, load_model
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve, auc
import pandas as pd
import time
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
from einops.layers.tensorflow import Rearrange

# Mount Google Drive
drive.mount('/content/drive')

# %% [markdown]
# ## 1. Load and Preprocess Data

# %%
# Load PneumoniaMNIST dataset
dataset_path = '/content/drive/My Drive/datasets/pneumoniamnist_224.npz'
data = np.load(dataset_path)

# Preprocess images
X_train = np.repeat(np.expand_dims(data['train_images'], -1), 3, axis=-1)
y_train = data['train_labels'].flatten()
X_val = np.repeat(np.expand_dims(data['val_images'], -1), 3, axis=-1)
y_val = data['val_labels'].flatten()

class_names = ['normal', 'pneumonia']

print(f"Training data shape: {X_train.shape}")
print(f"Validation data shape: {X_val.shape}")

# %% [markdown]
# ## 2. Implement Your Original MambaBlock

# %%
class MambaBlock(Layer):
    def __init__(
        self,
        d_state=16,
        d_conv=4,
        expand=2,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        self.d_inner = int(self.expand * self.d_model)

        self.norm = LayerNormalization()
        self.in_proj = Dense(self.d_inner * 2, activation=None)

        self.conv1d = Conv1D(
            filters=self.d_inner, kernel_size=self.d_conv, strides=1,
            padding="same",
            groups=self.d_inner, activation=None
        )

        self.dt_proj = Dense(self.d_inner, activation=None)
        self.B_proj = Dense(self.d_state, activation=None)
        self.C_proj = Dense(self.d_state, activation=None)
        self.out_proj = Dense(self.d_model, activation=None)

        A = tf.experimental.numpy.arange(1, self.d_state + 1, dtype=tf.float32)
        A = tf.tile(tf.expand_dims(A, 0), [self.d_inner, 1])
        self.A_log = self.add_weight(shape=A.shape, initializer=lambda shape, dtype: tf.math.log(A), trainable=True, name="A_log")
        self.D = self.add_weight(shape=(self.d_inner,), initializer="ones", trainable=True, name="D")

        super().build(input_shape)

    def ssm(self, x):
        A = -tf.exp(tf.cast(self.A_log, dtype=tf.float32))
        D = tf.cast(self.D, dtype=tf.float32)
        delta = tf.nn.softplus(self.dt_proj(x))
        B = self.B_proj(x)
        C = self.C_proj(x)

        dA = tf.exp(tf.einsum('b l d, d n -> b l d n', delta, A))
        dB = tf.einsum('b l d, b l n -> b l d n', delta, B)

        dA_transposed = tf.transpose(dA, perm=[1, 0, 2, 3])
        dB_transposed = tf.transpose(dB, perm=[1, 0, 2, 3])

        h = tf.scan(
            lambda h_prev, elems: h_prev * elems[0] + elems[1],
            (dA_transposed, dB_transposed),
            initializer=tf.zeros([tf.shape(x)[0], self.d_inner, self.d_state])
        )

        h = tf.transpose(h, perm=[1, 0, 2, 3])

        y = tf.einsum('b l d n, b l n -> b l d', h, C) + x * tf.expand_dims(D, axis=0)
        return y

    def call(self, x):
        x_residual = x
        x = self.norm(x)
        x_proj = self.in_proj(x)
        x_intermediate, gate = tf.split(x_proj, num_or_size_splits=2, axis=-1)
        x_conv = self.conv1d(x_intermediate)
        x_activated = tf.nn.silu(x_conv)
        y_ssm = self.ssm(x_activated)
        y_gated = y_ssm * tf.nn.silu(gate)
        y_out = self.out_proj(y_gated)
        return x_residual + y_out

    def get_config(self):
        config = super().get_config()
        config.update({
            "d_state": self.d_state,
            "d_conv": self.d_conv,
            "expand": self.expand,
        })
        return config

# %% [markdown]
# ## 3. Load Your Original MedMamba Model

# %%
def load_medmamba_model():
    try:
        model = load_model(
            '/content/drive/My Drive/saved_models/best_medmamba_model.keras',
            custom_objects={
                'MambaBlock': MambaBlock,
                'Rearrange': Rearrange
            }
        )
        print("✅ MedMamba model loaded successfully!")
        return model
    except Exception as e:
        print(f"❌ Failed to load MedMamba: {str(e)}")
        return None

medmamba = load_medmamba_model()

# %% [markdown]
# ## 4. Evaluate MedMamba

# %%
if medmamba:
    print("\n=== Evaluating MedMamba ===")
    start_time = time.time()
    y_pred_mamba = (medmamba.predict(X_val) > 0.5).astype(int).flatten()
    y_probs_mamba = medmamba.predict(X_val).flatten()
    infer_time_mamba = time.time() - start_time

    medmamba_results = {
        'Accuracy': accuracy_score(y_val, y_pred_mamba),
        'AUC': roc_auc_score(y_val, y_probs_mamba),
        'Inference Time (s)': infer_time_mamba,
    }

    print(f"Validation Accuracy: {medmamba_results['Accuracy']:.4f}")
    print(f"Validation AUC: {medmamba_results['AUC']:.4f}")
    print(f"Inference Time: {infer_time_mamba:.4f}s")
else:
    medmamba_results = None

# %% [markdown]
# ## 5. RankSVM Implementation

# %%
# VGG19 Feature Extractor
def create_feature_extractor():
    base_model = tf.keras.applications.VGG19(
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3)
    )
    base_model.trainable = False
    return tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D()
    ])

# Extract features
print("\nExtracting VGG19 features for RankSVM...")
feature_extractor = create_feature_extractor()
X_train_features = feature_extractor.predict(X_train, verbose=1)
X_val_features = feature_extractor.predict(X_val, verbose=1)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_features)
X_val_scaled = scaler.transform(X_val_features)

# Train SVMs
svm_models = {
    'RankSVM (Linear)': SVC(kernel='linear', probability=True, class_weight='balanced'),
    'RankSVM (RBF)': SVC(kernel='rbf', C=1, gamma='scale', probability=True, class_weight='balanced')
}

svm_results = {}

for name, model in svm_models.items():
    print(f"\n=== Training {name} ===")

    start_time = time.time()
    model.fit(X_train_scaled, y_train)
    train_time = time.time() - start_time

    start_time = time.time()
    y_pred = model.predict(X_val_scaled)
    y_probs = model.predict_proba(X_val_scaled)[:, 1]
    infer_time = time.time() - start_time

    svm_results[name] = {
        'Accuracy': accuracy_score(y_val, y_pred),
        'AUC': roc_auc_score(y_val, y_probs),
        'Train Time (s)': train_time,
        'Inference Time (s)': infer_time,
    }

    print(f"Validation Accuracy: {svm_results[name]['Accuracy']:.4f}")
    print(f"Validation AUC: {svm_results[name]['AUC']:.4f}")

# %% [markdown]
# ## 6. Results Comparison

# %%
# Prepare results
all_results = {
    'RankSVM (Linear)': svm_results['RankSVM (Linear)'],
    'RankSVM (RBF)': svm_results['RankSVM (RBF)']
}

if medmamba_results:
    all_results['MedMamba'] = medmamba_results

# Create comparison table
comparison_df = pd.DataFrame(all_results).T
print("\nModel Comparison:")
display(comparison_df)

# Visualization
plt.figure(figsize=(15, 5))
metrics = ['Accuracy', 'AUC', 'Inference Time (s)']

for i, metric in enumerate(metrics):
    plt.subplot(1, 3, i+1)
    values = [all_results[m][metric] for m in all_results]
    sns.barplot(x=list(all_results.keys()), y=values)
    plt.title(metric)
    plt.xticks(rotation=45)
    if metric == 'Inference Time (s)':
        plt.yscale('log')
    elif metric in ['Accuracy', 'AUC']:
        plt.ylim(0.8, 1.0)

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=300)
plt.show()

# %% [markdown]
# ## 7. Confusion Matrices and ROC Curves

# %%
if medmamba_results:
    # MedMamba Confusion Matrix
    plt.figure(figsize=(6, 6))
    cm = confusion_matrix(y_val, y_pred_mamba)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('MedMamba Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    # MedMamba ROC Curve
    fpr, tpr, _ = roc_curve(y_val, y_probs_mamba)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'MedMamba (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title('MedMamba ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.show()

# RankSVM Confusion Matrix and ROC
for name in ['RankSVM (Linear)', 'RankSVM (RBF)']:
    model = svm_models[name]
    y_pred = model.predict(X_val_scaled)
    y_probs = model.predict_proba(X_val_scaled)[:, 1]

    plt.figure(figsize=(6, 6))
    cm = confusion_matrix(y_val, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'{name} Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    fpr, tpr, _ = roc_curve(y_val, y_probs)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f'{name} ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.show()

# %% [markdown]
# ## 8. Save Results

# %%
# Save all results
results_df = pd.DataFrame(all_results).T
results_df.to_csv('model_comparison_results.csv')

from google.colab import files
files.download('model_comparison_results.csv')
files.download('model_comparison.png')

print("\n✅ All comparisons completed!")



# Cell 1: Installations & Imports (Run First)
!pip install seaborn pandas scikit-learn einops tensorflow-addons
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, load_model
from sklearn.ensemble import (RandomForestClassifier,
                            GradientBoostingClassifier,
                            AdaBoostClassifier)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')

print("✅ All packages installed and imported successfully!")

# Cell 2: Load Dataset (Run After Cell 1)
# Load PneumoniaMNIST dataset
dataset_path = '/content/drive/My Drive/datasets/pneumoniamnist_224.npz'
data = np.load(dataset_path)

# Preprocess images (match MedMamba's expected input format)
X_train = np.repeat(np.expand_dims(data['train_images'], -1), 3, axis=-1)
y_train = data['train_labels'].flatten()
X_val = np.repeat(np.expand_dims(data['val_images'], -1), 3, axis=-1)
y_val = data['val_labels'].flatten()

class_names = ['normal', 'pneumonia']

print(f"Training data shape: {X_train.shape}")
print(f"Validation data shape: {X_val.shape}")
print(f"Class names: {class_names}\n")
print("✅ Data loaded and preprocessed successfully!")

# Cell 3: MedMamba Implementation (Run After Cell 2)
class MambaBlock(Layer):
    def __init__(self, d_state=16, d_conv=4, expand=2, **kwargs):
        super().__init__(**kwargs)
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        self.d_inner = int(self.expand * self.d_model)

        self.norm = LayerNormalization()
        self.in_proj = Dense(self.d_inner * 2, activation=None)

        self.conv1d = Conv1D(
            filters=self.d_inner,
            kernel_size=self.d_conv,
            padding="same",
            groups=self.d_inner,
            activation=None
        )

        self.dt_proj = Dense(self.d_inner, activation=None)
        self.B_proj = Dense(self.d_state, activation=None)
        self.C_proj = Dense(self.d_state, activation=None)
        self.out_proj = Dense(self.d_model, activation=None)

        # State space model parameters
        A = tf.experimental.numpy.arange(1, self.d_state + 1, dtype=tf.float32)
        A = tf.tile(tf.expand_dims(A, 0), [self.d_inner, 1])
        self.A_log = self.add_weight(
            shape=A.shape,
            initializer=lambda shape, dtype: tf.math.log(A),
            trainable=True,
            name="A_log"
        )
        self.D = self.add_weight(
            shape=(self.d_inner,),
            initializer="ones",
            trainable=True,
            name="D"
        )
        super().build(input_shape)

    def ssm(self, x):
        A = -tf.exp(tf.cast(self.A_log, dtype=tf.float32))
        D = tf.cast(self.D, dtype=tf.float32)
        delta = tf.nn.softplus(self.dt_proj(x))
        B = self.B_proj(x)
        C = self.C_proj(x)

        dA = tf.exp(tf.einsum('b l d, d n -> b l d n', delta, A))
        dB = tf.einsum('b l d, b l n -> b l d n', delta, B)

        dA_transposed = tf.transpose(dA, perm=[1, 0, 2, 3])
        dB_transposed = tf.transpose(dB, perm=[1, 0, 2, 3])

        h = tf.scan(
            lambda h_prev, elems: h_prev * elems[0] + elems[1],
            (dA_transposed, dB_transposed),
            initializer=tf.zeros([tf.shape(x)[0], self.d_inner, self.d_state])
        )

        h = tf.transpose(h, perm=[1, 0, 2, 3])
        y = tf.einsum('b l d n, b l n -> b l d', h, C) + x * tf.expand_dims(D, axis=0)
        return y

    def call(self, x):
        x_residual = x
        x = self.norm(x)
        x_proj = self.in_proj(x)
        x_intermediate, gate = tf.split(x_proj, num_or_size_splits=2, axis=-1)
        x_conv = self.conv1d(x_intermediate)
        x_activated = tf.nn.silu(x_conv)
        y_ssm = self.ssm(x_activated)
        y_gated = y_ssm * tf.nn.silu(gate)
        y_out = self.out_proj(y_gated)
        return x_residual + y_out

    def get_config(self):
        config = super().get_config()
        config.update({
            "d_state": self.d_state,
            "d_conv": self.d_conv,
            "expand": self.expand,
        })
        return config

def load_medmamba():
    try:
        model = load_model(
            '/content/drive/My Drive/saved_models/best_medmamba_model.keras',
            custom_objects={'MambaBlock': MambaBlock}
        )
        print("✅ MedMamba model loaded successfully!")
        return model
    except Exception as e:
        print(f"❌ Failed to load MedMamba: {str(e)}")
        return None

print("\nMedMamba components defined successfully!")

# Cell 4: Comparison Models (Run after Cells 1-3)
# ==============================================
# 1. Simplified Graph Fusion Model (no stellargraph needed)
# --------------------------------------------------------
def create_graph_fusion_model(input_shape):
    # Dual input branches
    input_ct = Input(shape=input_shape)
    input_cxr = Input(shape=input_shape)

    # Feature processing
    x = Concatenate()([input_ct, input_cxr])
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    outputs = Dense(1, activation='sigmoid')(x)

    return Model(inputs=[input_ct, input_cxr], outputs=outputs, name='GraphFusion')

# 2. LSTM Model
# -------------
def create_lstm_model(input_shape):
    return tf.keras.Sequential([
        Reshape((-1, input_shape[0])),
        LSTM(128, return_sequences=True),
        Dropout(0.3),
        LSTM(64),
        Dense(1, activation='sigmoid')
    ], name='LSTM_Model')

# 3. Fuzzy Logic Model
# --------------------
class FuzzyClassificationLayer(Layer):
    def __init__(self, num_classes=2, **kwargs):
        super().__init__(**kwargs)
        self.num_classes = num_classes

    def build(self, input_shape):
        self.membership_kernels = self.add_weight(
            shape=(input_shape[-1], self.num_classes),
            initializer='glorot_uniform',
            name='fuzzy_weights'
        )
        super().build(input_shape)

    def call(self, inputs):
        # Gaussian fuzzy membership (simplified)
        distances = tf.square(tf.expand_dims(inputs, -1) - self.membership_kernels)
        membership = tf.exp(-distances)
        return tf.reduce_mean(membership, axis=1)

def create_fuzzy_model(input_shape):
    inputs = Input(shape=input_shape)
    x = Dense(128, activation='relu')(inputs)
    x = FuzzyClassificationLayer()(x)
    outputs = Dense(1, activation='sigmoid')(x)
    return Model(inputs, outputs, name='FuzzyModel')

# 4. VGG19 Feature Extractor (for traditional ML models)
# -----------------------------------------------------
def create_vgg_feature_extractor():
    base_model = tf.keras.applications.VGG19(
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3)
    )
    base_model.trainable = False
    return Model(
        inputs=base_model.input,
        outputs=GlobalAveragePooling2D()(base_model.output),
        name='VGG_Feature_Extractor'
    )

# 5. Traditional ML Models
# ------------------------
def initialize_ml_models():
    return {
        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
        'GradientBoosting': GradientBoostingClassifier(random_state=42),
        'AdaBoost': AdaBoostClassifier(random_state=42),
        'KNN': KNeighborsClassifier(n_neighbors=5),
        'GaussianNB': GaussianNB()
    }

print("""
✅ Comparison models defined:
1. Graph Fusion (simplified)
2. LSTM Model
3. Fuzzy Logic Model
4. VGG19 Feature Extractor
5. Traditional ML Models (RF, GBM, AdaBoost, KNN, Naive Bayes)
""")

def evaluate_all_models(X_train, y_train, X_val, y_val):
    results = {}

    # 1. MedMamba Evaluation
    medmamba = load_medmamba()
    if medmamba:
        y_pred = (medmamba.predict(X_val) > 0.5).astype(int)
        results['MedMamba'] = {
            'Accuracy': accuracy_score(y_val, y_pred),
            'AUC': roc_auc_score(y_val, medmamba.predict(X_val).flatten()),
            'Type': 'Neural Network'
        }

    # 2. Feature Extraction
    vgg = tf.keras.applications.VGG19(
        include_top=False,
        weights='imagenet',
        input_shape=X_train.shape[1:]
    )
    features_train = vgg.predict(X_train)
    features_val = vgg.predict(X_val)
    features_train = features_train.reshape(features_train.shape[0], -1)
    features_val = features_val.reshape(features_val.shape[0], -1)

    # 3. Graph Fusion - FIXED: using create_graph_fusion_model instead of graph_based_fusion
    graph_model = create_graph_fusion_model((features_train.shape[1],))
    graph_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    graph_model.fit(
        [features_train, features_train], y_train,
        epochs=10, batch_size=32, verbose=0
    )
    y_pred = (graph_model.predict([features_val, features_val]) > 0.5).astype(int)
    results['Graph Fusion'] = {
        'Accuracy': accuracy_score(y_val, y_pred),
        'AUC': roc_auc_score(y_val, graph_model.predict([features_val, features_val])),
        'Type': 'Neural Network'
    }

    # 4. LSTM
    lstm_model = create_lstm_model(features_train.shape[1:])
    lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    lstm_model.fit(
        features_train, y_train,
        epochs=10, batch_size=32, verbose=0
    )
    y_pred = (lstm_model.predict(features_val) > 0.5).astype(int)
    results['LSTM'] = {
        'Accuracy': accuracy_score(y_val, y_pred),
        'AUC': roc_auc_score(y_val, lstm_model.predict(features_val)),
        'Type': 'Neural Network'
    }

    # 5. Fuzzy Model
    fuzzy_model = create_fuzzy_model(features_train.shape[1:])
    fuzzy_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    fuzzy_model.fit(
        features_train, y_train,
        epochs=10, batch_size=32, verbose=0
    )
    y_pred = (fuzzy_model.predict(features_val) > 0.5).astype(int)
    results['Fuzzy Model'] = {
        'Accuracy': accuracy_score(y_val, y_pred),
        'AUC': roc_auc_score(y_val, fuzzy_model.predict(features_val)),
        'Type': 'Fuzzy Logic'
    }

    # 6. Ensemble Methods
    ensembles = initialize_ml_models()  # Changed from train_ensemble_classifiers to initialize_ml_models
    for name, model in ensembles.items():
        model.fit(features_train, y_train)  # Need to train the models first
        y_pred = model.predict(features_val)
        results[name] = {
            'Accuracy': accuracy_score(y_val, y_pred),
            'AUC': roc_auc_score(y_val, model.predict_proba(features_val)[:,1] if hasattr(model, 'predict_proba') else model.decision_function(features_val)),
            'Type': 'Traditional ML'
        }

    return pd.DataFrame(results).T.sort_values('AUC', ascending=False)

# Run comparison
results_df = evaluate_all_models(X_train, y_train, X_val, y_val)
display(results_df)

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define save paths
import os
drive_path = '/content/drive/MyDrive/model_results/'  # Change to your preferred folder
os.makedirs(drive_path, exist_ok=True)

# Save results dataframe
results_csv_path = os.path.join(drive_path, 'model_results.csv')
results_df.to_csv(results_csv_path)
print(f"Results saved to Google Drive: {results_csv_path}")

import matplotlib.pyplot as plt
import seaborn as sns

def save_comparison_plots(results_df, save_dir):
    plt.figure(figsize=(14, 6))
    sns.set_style("whitegrid")

    # Accuracy Plot
    plt.subplot(1, 2, 1)
    sns.barplot(x=results_df.index, y='Accuracy', hue='Type', data=results_df.reset_index())
    plt.title('Model Accuracy Comparison')
    plt.xticks(rotation=45)
    plt.ylim(0.7, 1.0)

    # AUC Plot
    plt.subplot(1, 2, 2)
    sns.barplot(x=results_df.index, y='AUC', hue='Type', data=results_df.reset_index())
    plt.title('Model AUC Comparison')
    plt.xticks(rotation=45)
    plt.ylim(0.7, 1.0)

    plt.tight_layout()

    # Save plot
    plot_path = os.path.join(save_dir, 'model_comparison.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()  # Close the figure to free memory
    print(f"Comparison plots saved to: {plot_path}")

# Save plots to Google Drive
save_comparison_plots(results_df, drive_path)

# 1. Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 2. Define paths
import os
drive_path = '/content/drive/MyDrive/model_results/'
os.makedirs(drive_path, exist_ok=True)

# 3. Check for existing results
results_csv_path = os.path.join(drive_path, 'model_results.csv')
if os.path.exists(results_csv_path):
    print("Loading existing results...")
    results_df = pd.read_csv(results_csv_path, index_col=0)
else:
    print("Running model evaluations...")
    results_df = evaluate_all_models(X_train, y_train, X_val, y_val)
    results_df.to_csv(results_csv_path)
    print(f"Results saved to {results_csv_path}")

# 4. Generate and save plots
save_comparison_plots(results_df, drive_path)

# 5. Optionally display the plots
plt.figure(figsize=(14, 6))
sns.set_style("whitegrid")

# Accuracy Plot
plt.subplot(1, 2, 1)
sns.barplot(x=results_df.index, y='Accuracy', hue='Type', data=results_df.reset_index())
plt.title('Model Accuracy Comparison')
plt.xticks(rotation=45)
plt.ylim(0.7, 1.0)

# AUC Plot
plt.subplot(1, 2, 2)
sns.barplot(x=results_df.index, y='AUC', hue='Type', data=results_df.reset_index())
plt.title('Model AUC Comparison')
plt.xticks(rotation=45)
plt.ylim(0.7, 1.0)

plt.tight_layout()
plt.show()